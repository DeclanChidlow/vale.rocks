---
lesswrong_link: https://www.lesswrong.com/posts/sp82P8D3W3hB44wpw/vale-s-shortform?commentId=WbbHwmzmwex4XB67v
---

Predicting AGI/ASI timelines is highly speculative and unviable. Ultimately, there are too many unknowns and complex variables at play. Any timeline must deal with systems and consequences multiple steps out, where tiny initial errors compound dramatically. A range can be somewhat reasonable, a more specific figure less so, and accurately predicting the consequences of the final event when it comes to pass even further improbable. It is simply impractical to come up with an accurate timeline with the knowledge we currently have.

Despite this, timelines are popular – both with the general AI hype crowd and those more informed. People don't seem to penalise incorrect timelines – as evidenced by the many predicted dates we've seen pass without event. Thus, there's little downside to proposing a timeline, even an outrageous one. If it's wrong, it's largely forgotten. If it's right, you're lauded a prophet. The nebulous definitions of 'AGI' and 'ASI' also offer an out. One can always argue the achieved system doesn't meet their specific definition or point to the [AI Effect](https://en.wikipedia.org/wiki/AI_effect).

I suppose Gwern's fantastic work on [The Scaling Hypothesis](https://gwern.net/scaling-hypothesis) is evidence of how an accurate prediction can significantly boost credibility and personal notoriety. Proposing timelines gets attention. Anyone noteworthy with a timeline becomes the centre of discussion, especially if their proposal is on the extremes of the spectrum.

The incentives for making timeline predictions seem heavily weighted towards upside, regardless of the actual predictive power or accuracy. Plenty to gain; not much to lose.
